1. Original state of the project is aimed to be run with: docker
In order to do it from cmd: access elasticGrpc foler (cd elasticGrpc)
  and enter command: docker-compose up --build

2. If you want to run the project on the local machine, you need to do some changes in
    project
    a. data_ingest_worker/adapters/db/db.go in method: func (a *Adapter) Get() ([]domain.Ad, error)
        //	file, err := os.Open("adapters/db/data.json") // using localhost with make run from Makefila
        	file, err := os.Open("data.json") // using docker container

        Comment or uncomment according to your choise
    b. data_store_service/internal/adapters/db/db.go in: func NewAdapter() (*Adapter, error)
        Addresses: []string{
        			//	"http://localhost:9200", // for localhost with make run from Makefile
        			"http://elasticsearch:9200", // for docker
        		}
        Comment or uncomment for your choise
    c. data_store_service/cmd/main in  func main()
        //err = os.Setenv("DATA_INGEST_WORKER_URL", "127.0.0.1:4001")
        	err = os.Setenv("DATA_INGEST_WORKER_URL", "grpcserver:4001")
        	Comment or uncomment according to your choise

    Note
        If your choise is to run the project from localhost, uncomment/comment all (a, b, c)
        1.open in cmd elasticGrpc/elasticsearchDock and execute cmd:
            a.  docker build -t my-elasticsearch-image1 .
            b.  docker run -d -p 9200:9200 --name my-elasticsearch-container my-elasticsearch-image1

        2.open in cmd elasticGrpc/data_ingest_worker folder and execute command: make run
        3.open in cmd elasticGrpc/data_store_service folder and execute command: make run

3. How to use after running the proj:
    a) from postman browser:
        http://localhost:8080/seeddata // optional

        http://localhost:8080/create  // data_store_service takes data from data_ingest_worker and store it in ES
                                      // Receives and stores data in Elasticsearch.

        http://localhost:8080/getalldocs // Return all documents

        http://localhost:8080/searchTitle?title=Продается // ex -> change title according to your search condition
                                                         // Supports full-text search by title field, considering Russian and Romanian morphology.

        http://localhost:8080/scroll?from=1&size=5       // ex -> change from and size according to your search condition
                                                         // returns data with "pagination"

        http://localhost:8080/aggsub  // Returns aggregated data by subcategory field.

    b) if you want to use the project from postman
        You can check the following endpoints, but first go in Basic Auth/Username:elastic/Password:ELASTIC_PASSWORD
        GET http://localhost:9200/_cat/indices // you should receive: adv Index (Table)
        GET http://localhost:9200/adv/_doc/38118545 // choise your id according to Documents (Rows)
        PUT http://localhost:9200/adv/_doc/123 // in body chose raw JSON example of document (row)
                {
                  "id": "38118111",
                  "categories": {
                   "subcategory": "1407"
                  },
                  "title": {
                   "ro": "Teren sub constructie in apropiere de Vadul lui Voda",
                   "ru": "Teren sub constructie in apropiere de Vadul lui Voda"
                  },
                  "type": "standard",
                  "posted": 1486556302.101039
                 }
        GET http://localhost:9200/adv/_search?size=53 // default size it 10, chose as you need
        GET http://localhost:9200/adv/_search // returns all docs(rows) from indice adv (table)
        GET http://localhost:9200/adv/_search?from=1&size=8
        GET http://localhost:9200/adv/_search // in body raw JSON:
             {
              "size": 0,
                  "aggs": {
                    "subcategories": {
                      "terms": {
                        "field": "categories.subcategory.keyword"
                      }
                    }
                  }
              }


4. How project is organized:
    The project has 3 components:
            a. data_ingest_worker grpc server runs on :4001
            b. data_store_service http server runs on :8080
            c elasticsearch db runs on :9200
    Architecture of the project is: Hexagonal, with ports allows external actors to use, implement, and orchestrate
                                               business logic
    data_ingest_worker creates a grpc server on port :4001 wich has: service RetrieveData {
                                                                       rpc GetData(Empty) returns (GetDataResponse);
                                                                     }
    ***Important Note*** proto files I create an external library in order to allow library to be used and from other projects
    https://github.com/w3gop2p/elasticGrpc-proto
    in data_ingest_worker basically data is read from json files and is returned: as GetDataResponse, as you can see in rpc GetData

    next in data_store_worker, is created connection with elastic search, but also a stub (client) of grpc server allowing to access its methods,
    with the help of the client for grpc server, data is saved in elasticsearch database

    Shortly about architecture: adapters implements ports, and application are using adaptors

 5. Testing
        Is tested http server with all its methods, using a moq object
        open in cmd elasticGrpc/data_store_service: make test
        for further implementation of testing of grpc service using mocks/also using automatic mock generation
        vektra/mockery



